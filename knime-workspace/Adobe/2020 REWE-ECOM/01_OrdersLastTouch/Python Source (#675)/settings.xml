<?xml version="1.0" encoding="UTF-8"?>
<config xmlns="http://www.knime.org/2008/09/XMLConfig" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.knime.org/2008/09/XMLConfig http://www.knime.org/XMLConfig_2008_09.xsd" key="settings.xml">
<entry key="node_file" type="xstring" value="settings.xml"/>
<config key="flow_stack"/>
<config key="internal_node_subsettings">
<entry key="memory_policy" type="xstring" value="CacheSmallInMemory"/>
</config>
<config key="model">
<entry key="sourceCode" type="xstring" value="# coding: utf-8%%00010&quot;&quot;&quot; 2020-07-30 MC:  Adobe API 1.4 Functions %%00010&quot;&quot;&quot;%%00010import os%%00010import copy%%00010import requests%%00010import json%%00010import datetime%%00010import time%%00010import uuid%%00010import binascii%%00010import hashlib%%00010import logging%%00010import pandas as pd%%00010import numpy as np%%00010from math import ceil%%00010from dateutil.relativedelta import relativedelta%%00010%%00010# Logger setup%%00010log = logging.getLogger()%%00010log.addHandler(logging.StreamHandler())%%00010log.setLevel(logging.INFO)%%00010%%00010# For use in Knime workflow (True) or direct run (False)%%00010KNIME = True%%00010%%00010# Frist day to get data for%%00010MANUAL_FIRST_EMPTY_DAY = &quot;2020-08-24&quot;%%00010# Last day to get data for, None for yesterday%%00010LAST_QUERY_DAY = None%%00010%%00010if KNIME:%%00010    USER = flow_variables[&quot;ADOBE_USER&quot;]%%00010    PW = flow_variables[&quot;ADOBE_PW&quot;]%%00010    PROXY = flow_variables.get(&quot;PROXY&quot;)%%00010    # If we dont have data yet (setup of flow) manually set first day%%00010    DATE_FIRST_EMPTY_DAY = flow_variables.get(&quot;last_entry&quot;, MANUAL_FIRST_EMPTY_DAY)%%00010    TEST = flow_variables.get(&quot;TEST&quot;)  # Set 0 for production%%00010else:%%00010    # Set those in your environment variables%%00010    USER = os.environ.get(&quot;ADOBE_USER&quot;)%%00010    PW = os.environ.get(&quot;ADOBE_PW&quot;)%%00010    PROXY = None%%00010    DATE_FIRST_EMPTY_DAY = MANUAL_FIRST_EMPTY_DAY%%00010%%00010PROXIES = None%%00010if PROXY is not None and PROXY != &quot;None&quot;:%%00010    PROXIES = {&quot;https&quot;: f&quot;{PROXY}&quot;}%%00010    %%00010API_URL = &quot;https://api.omniture.com/admin/1.4/rest/&quot;%%00010QUEUE_TIMEOUT = 3%%00010QUEUE_MAX_WAIT_TIME = 3600%%00010MAX_RESPONSE_ERRORS = 10%%00010ERROR_TIMEOUT = 10%%00010%%00010################################################################################%%00010# DEFINE QUERY%%00010################################################################################%%00010%%00010# DEFINE QUERY  -&gt; empty = []%%00010REPORT_SUITE = &quot;rewrewededev&quot;%%00010%%00010# Granularity of Data: day, week, month%%00010GRANULARITY = &quot;day&quot;%%00010%%00010# Reduce query size. None for default request. Needed to reduce timeouts%%00010LINES_PER_REQUEST = 200%%00010# How many requests to send at once%%00010BATCH_SIZE = 30%%00010%%00010METRICS = [{&quot;id&quot;: &quot;orders&quot;}]%%00010SEGMENTS = []%%00010DIMENSIONS = [%%00010    {&quot;id&quot;: &quot;evar72&quot;, &quot;top&quot;: LINES_PER_REQUEST},%%00010    {&quot;id&quot;: &quot;lasttouchchannel&quot;},%%00010    {&quot;id&quot;: &quot;lasttouchchanneldetail&quot;},%%00010]%%00010%%00010PERIOD_FREQ = &quot;7D&quot;%%00010if GRANULARITY == &quot;week&quot;:%%00010    PERIOD_FREQ = &quot;W-MON&quot;%%00010if GRANULARITY == &quot;month&quot;:%%00010    PERIOD_FREQ = &quot;M&quot;%%00010if LINES_PER_REQUEST:%%00010    PERIOD_FREQ = &quot;1D&quot;%%00010%%00010# Query Template%%00010QUERY = {%%00010    &quot;reportDescription&quot;: {%%00010        &quot;reportSuiteID&quot;: REPORT_SUITE,%%00010        &quot;metrics&quot;: METRICS,%%00010        &quot;segments&quot;: SEGMENTS,%%00010        &quot;elements&quot;: DIMENSIONS,%%00010        &quot;dateGranularity&quot;: GRANULARITY,%%00010        &quot;dateFrom&quot;: None,%%00010        &quot;dateTo&quot;: None,%%00010    }%%00010}%%00010# ------------------------------------------------------------------------------%%00010%%00010def parse(raw_response):%%00010    log.info(&quot;Parsing raw json response.&quot;)%%00010    report = raw_response[&quot;report&quot;]%%00010    raw_data = report[&quot;data&quot;]%%00010%%00010    dimensions, metrics = _parse_header(report)%%00010    data = _parse_data(raw_data, metric_count=len(metrics))%%00010    header = _fix_header(dimensions, metrics, data)%%00010    return pd.DataFrame(data, columns=header)%%00010%%00010%%00010def _parse_header(report):%%00010    log.debug(&quot;Parsing dimensions and metrics.&quot;)%%00010    dimensions = [%%00010        _classification_or_name(dimension) for dimension in report[&quot;elements&quot;]%%00010    ]%%00010    metrics = [metric[&quot;name&quot;] for metric in report[&quot;metrics&quot;]]%%00010    return dimensions, metrics%%00010%%00010%%00010def _classification_or_name(element):%%00010    if &quot;classification&quot; in element:%%00010        return element[&quot;classification&quot;]%%00010    return element[&quot;name&quot;]%%00010%%00010%%00010def _parse_data(data, metric_count):%%00010    &quot;&quot;&quot;%%00010    Recursive parsing of the &quot;data&quot; part of the Adobe response.%%00010    :param data: list of dicts and lists. quite a complicated structure%%00010    :param metric_count: int, number of metrics in report%%00010    :return: list of lists%%00010    &quot;&quot;&quot;%%00010    log.debug(&quot;Parsing report data (recursively).&quot;)%%00010    if len(data) &gt; 0 and &quot;breakdown&quot; in data[0]:%%00010        rows = list()%%00010        for chunk in data:%%00010            dim_value = _dimension_value(chunk)%%00010            rows += [%%00010                [dim_value] + row%%00010                for row in _parse_data(chunk[&quot;breakdown&quot;], metric_count)%%00010            ]%%00010        return rows%%00010    else:%%00010        return _parse_most_granular(data, metric_count)%%00010%%00010%%00010def _parse_most_granular(data, metric_count):%%00010    &quot;&quot;&quot;%%00010    Parsing of the most granular part of the response.%%00010    It is different depending on if there's a granularity breakdown or not%%00010    :param data: dict%%00010    :param metric_count: int, number of metrics in report%%00010    :return: list of lists%%00010    &quot;&quot;&quot;%%00010    log.debug(&quot;Parsing most granular level of data.&quot;)%%00010    rows = list()%%00010    for chunk in data:%%00010        part_rows = [(val if val != &quot;&quot; else np.nan) for val in chunk[&quot;counts&quot;]]%%00010        # data alignment is a bit different if adding granularity breakdowns%%00010        if len(chunk[&quot;counts&quot;]) &gt; metric_count:%%00010            part_rows = more_itertools.chunked(iterable=part_rows, n=metric_count + 1)%%00010        else:%%00010            part_rows = [part_rows]%%00010%%00010        dim_value = _dimension_value(chunk)%%00010        rows += [[dim_value] + part_row for part_row in part_rows]%%00010    return rows%%00010%%00010%%00010def _dimension_value(chunk):%%00010    if _dimension_value_is_nan(chunk):%%00010        return np.nan%%00010    elif &quot;year&quot; in chunk:%%00010        return _to_datetime(chunk)%%00010    else:%%00010        return chunk[&quot;name&quot;]%%00010%%00010%%00010def _dimension_value_is_nan(chunk):%%00010    return (%%00010        (&quot;name&quot; not in chunk)%%00010        or (chunk[&quot;name&quot;] == &quot;&quot;)%%00010        or (chunk[&quot;name&quot;] == &quot;::unspecified::&quot;)%%00010    )%%00010%%00010%%00010def _to_datetime(chunk):%%00010    time_stamp = datetime.datetime(%%00010        year=chunk[&quot;year&quot;],%%00010        month=chunk[&quot;month&quot;],%%00010        day=chunk[&quot;day&quot;],%%00010        hour=chunk.get(&quot;hour&quot;, 0),%%00010    )%%00010    return time_stamp.strftime(&quot;%Y-%m-%d %H:00:00&quot;)%%00010%%00010%%00010def _fix_header(dimensions, metrics, data):%%00010    header = dimensions + metrics%%00010    if len(header) != len(data[0]):  # can only be when granularity breakdown is used%%00010        return [&quot;Datetime&quot;] + header%%00010    return header%%00010%%00010%%00010def build_headers():%%00010    &quot;&quot;&quot; Create WSSE compliant headers for auth -&gt; see api doc%%00010    &quot;&quot;&quot;%%00010    nonce = str(uuid.uuid4())%%00010    base64nonce = binascii.b2a_base64(binascii.a2b_qp(nonce))%%00010    created_date = datetime.datetime.utcnow().isoformat() + &quot;Z&quot;%%00010    sha = nonce + created_date + PW%%00010    sha_object = hashlib.sha1(sha.encode())%%00010    password_64 = binascii.b2a_base64(sha_object.digest())%%00010%%00010    properties = {%%00010        &quot;Username&quot;: USER,%%00010        &quot;PasswordDigest&quot;: password_64.decode().strip(),%%00010        &quot;Nonce&quot;: base64nonce.decode().strip(),%%00010        &quot;Created&quot;: created_date,%%00010    }%%00010%%00010    header = ['{key}=&quot;{value}&quot;'.format(key=k, value=v) for k, v in properties.items()]%%00010    header = &quot;UsernameToken &quot; + &quot;, &quot;.join(header)%%00010    return {&quot;X-WSSE&quot;: header}%%00010%%00010%%00010def queue_report(query=QUERY, errors=0):%%00010    &quot;&quot;&quot; Call Report.Queue method and return the reportID %%00010    &quot;&quot;&quot;%%00010    header = build_headers()%%00010    query = json.loads(query)%%00010    if not query.get(&quot;reportDescription&quot;).get(&quot;dateTo&quot;):%%00010        query[&quot;reportDescription&quot;][&quot;dateTo&quot;] = query[&quot;reportDescription&quot;][&quot;dateFrom&quot;]%%00010    date_from = query.get(&quot;reportDescription&quot;).get(&quot;dateFrom&quot;)%%00010    date_to = query.get(&quot;reportDescription&quot;).get(&quot;dateTo&quot;)%%00010%%00010    try:%%00010        r = requests.post(%%00010            API_URL,%%00010            params={&quot;method&quot;: &quot;Report.Queue&quot;},%%00010            data=json.dumps(query),%%00010            headers=header,%%00010            proxies=PROXIES,%%00010        )%%00010    except Exception as e:%%00010        log.error(f&quot;Could not post queue request: {e}&quot;)%%00010        if errors &lt;= MAX_RESPONSE_ERRORS:%%00010            log.warning(f&quot;Retrying. Errors: {errors} / {MAX_RESPONSE_ERRORS}&quot;)%%00010            time.sleep(ERROR_TIMEOUT * errors)%%00010            return queue_report(reportID, i, errors + 1)%%00010        else:%%00010            log.error(f&quot;Too many errors: {MAX_RESPONSE_ERRORS}. Aborting: {e}&quot;)%%00010            return None%%00010    else:%%00010        if r.status_code == 200:%%00010            report_id = r.json().get(&quot;reportID&quot;)%%00010            log.info(%%00010                &quot;Successfully sent to queue. ReportID: {}. From {} to {}&quot;.format(%%00010                    report_id, date_from, date_to%%00010                )%%00010            )%%00010            return report_id%%00010        else:%%00010            log.error(%%00010                &quot;Invalid response. Status: {}. Response {}&quot;.format(%%00010                    r.status_code, r.text%%00010                )%%00010            )%%00010            return None%%00010%%00010def get_report(reportID, i=0, errors=0):%%00010    &quot;&quot;&quot;Call Report.Get method and return the report data&quot;&quot;&quot;%%00010    header = build_headers()%%00010    try:%%00010        r = requests.post(%%00010            API_URL,%%00010            params={&quot;method&quot;: &quot;Report.Get&quot;},%%00010            data=json.dumps({&quot;reportID&quot;: &quot;{}&quot;.format(reportID), &quot;page&quot;: 1}),%%00010            headers=header,%%00010            proxies=PROXIES,%%00010        )%%00010    except Exception as e:%%00010        log.error(f&quot;Could not post get request: {e}&quot;)%%00010        if errors &lt;= MAX_RESPONSE_ERRORS:%%00010            log.warning(f&quot;Retrying. Errors: {errors} / {MAX_RESPONSE_ERRORS}&quot;)%%00010            time.sleep(ERROR_TIMEOUT * errors)%%00010            return get_report(reportID, i, errors + 1)%%00010        else:%%00010            log.error(f&quot;Too many errors: {MAX_RESPONSE_ERRORS}. Aborting: {e}&quot;)%%00010            return None%%00010    else:%%00010        if r.status_code == 200:%%00010            log.info(f&quot;Report returned successfully: {reportID}&quot;)%%00010            return r.json()%%00010        else:%%00010            log.error(%%00010                &quot;Invalid response. Status: {}. Response {}&quot;.format(%%00010                    r.status_code, r.text%%00010                )%%00010            )%%00010            try:%%00010                if r.json().get(&quot;error&quot;) == &quot;report_not_ready&quot;:%%00010                    time_wait = QUEUE_TIMEOUT * i%%00010                    if time_wait &lt;= QUEUE_MAX_WAIT_TIME:%%00010                        log.warning(%%00010                            &quot;Report not ready. Retrying in {} s&quot;.format(time_wait)%%00010                        )%%00010                        time.sleep(time_wait)%%00010                        return get_report(reportID, i + 1)%%00010                    else:%%00010                        log.error(&quot;Queue wait too long. Report not retrieved.&quot;)%%00010                        return None%%00010            except:%%00010                if errors &lt;= MAX_RESPONSE_ERRORS:%%00010                    log.warning(f&quot;Retrying. Errors: {errors} / {MAX_RESPONSE_ERRORS}&quot;)%%00010                    time.sleep(ERROR_TIMEOUT * errors)%%00010                    return get_report(reportID, i, errors + 1)%%00010                else:%%00010                    log.error(f&quot;Too many errors: {MAX_RESPONSE_ERRORS}. Aborting: {e}&quot;)%%00010                    return None%%00010            if errors &lt;= MAX_RESPONSE_ERRORS:%%00010                log.warning(f&quot;Retrying. Errors: {errors} / {MAX_RESPONSE_ERRORS}&quot;)%%00010                time.sleep(ERROR_TIMEOUT * errors)%%00010                return get_report(reportID, i, errors + 1)%%00010            else:%%00010                log.error(f&quot;Too many errors: {MAX_RESPONSE_ERRORS}&quot;)%%00010                return None%%00010%%00010%%00010def data_to_df(report_data):%%00010    &quot;&quot;&quot; Parse and pre-process Data%%00010    &quot;&quot;&quot;%%00010    data = parse(report_data)%%00010    data[&quot;load_timestamp&quot;] = datetime.datetime.now()%%00010    return data%%00010# ------------------------------------------------------------------------------%%00010%%00010def get_request_totals(start_dates, end_dates):%%00010    &quot;&quot;&quot; For large requests, get number of total results before requesting all%%00010        Split query to batches according to this%%00010    &quot;&quot;&quot;%%00010    reports = []%%00010    num_results = []%%00010    log.info(&quot;Getting total results for request&quot;)%%00010%%00010    query = copy.deepcopy(QUERY)%%00010    dim = copy.deepcopy(DIMENSIONS)%%00010    dim[0][&quot;top&quot;] = 1%%00010    query[&quot;reportDescription&quot;][&quot;elements&quot;] = dim%%00010%%00010    for date_from, date_to in zip(start_dates, end_dates):%%00010        log.info(f&quot;Getting Total numbers for {date_from}&quot;)%%00010        if not date_to:%%00010            date_to = date_from%%00010        query[&quot;reportDescription&quot;][&quot;dateFrom&quot;] = date_from%%00010        query[&quot;reportDescription&quot;][&quot;dateTo&quot;] = date_to%%00010        log.info(f&quot;Query: {query}&quot;)%%00010        report_id = queue_report(json.dumps(query))%%00010        reports.append(report_id)%%00010    for report in reports:%%00010        report_data = get_report(report)%%00010        num = float(report_data.get(&quot;report&quot;).get(&quot;totals&quot;)[0])%%00010        log.info(f&quot;Total results for report {date_from} is: {num}&quot;)%%00010        num_results.append(num)%%00010    return num_results%%00010%%00010%%00010def make_queries(start_dates, end_dates, total_results):%%00010    &quot;&quot;&quot; create all queries - deal with pagination%%00010        :returns list of all queries%%00010    &quot;&quot;&quot;%%00010    queries = []%%00010    query = copy.deepcopy(QUERY)%%00010    dim = copy.deepcopy(DIMENSIONS)%%00010    if not total_results:%%00010        num_batches = [1 for x in start_dates]%%00010    else:%%00010        num_batches = [ceil(x / LINES_PER_REQUEST) for x in total_results]%%00010    total_requests = np.sum(num_batches)%%00010    for date_from, date_to, batches in zip(start_dates, end_dates, num_batches):%%00010        for batch in range(batches):%%00010            startingWith = (batch * LINES_PER_REQUEST) + 1%%00010            dim[0][&quot;startingWith&quot;] = startingWith%%00010            query[&quot;reportDescription&quot;][&quot;elements&quot;] = dim%%00010            query[&quot;reportDescription&quot;][&quot;dateFrom&quot;] = date_from%%00010            query[&quot;reportDescription&quot;][&quot;dateTo&quot;] = date_to%%00010            queries.append(json.dumps(query))%%00010    return queries%%00010%%00010%%00010def batchwise_request(queries):%%00010    &quot;&quot;&quot; Process queries batchwise, convert results to df%%00010        :returns DataFrame with query results%%00010    &quot;&quot;&quot;%%00010    clean_data = pd.DataFrame()%%00010    reports_all = []%%00010    results = []%%00010   %%00010    #for i in range(0, 8, BATCH_SIZE):%%00010    for i in range(0, len(queries), BATCH_SIZE):%%00010        log.info(f&quot;Iteration: {i+1} / {len(queries)}&quot;)%%00010        reports_batch = []%%00010        batch = queries[i : i + BATCH_SIZE]%%00010        for j, query in enumerate(batch):%%00010            log.info(f&quot;Sending request: {(i+1)+(j)}&quot;)%%00010            report_id = queue_report(query)%%00010            reports_all.append(report_id)%%00010            reports_batch.append(report_id)%%00010        for report in reports_batch:%%00010            data = get_report(report)%%00010            if data:%%00010%%00009            if float(data.get(&quot;report&quot;).get(&quot;totals&quot;)[0]) &gt; 0:%%00010%%00009                results.append(data)%%00010%%00009                data_df = data_to_df(data)%%00010%%00009                clean_data = clean_data.append(data_df)%%00010%%00009                # clean_data.to_csv(&quot;./2020-08-24_adobedata_08_10.csv&quot;, index=False)%%00009                %%00010%%00009                continue%%00010            log.error(f&quot;Request for {report} returned empty&quot;)%%00010    return clean_data%%00010%%00010%%00010if not LAST_QUERY_DAY:%%00010    last_query_day = pd.datetime.today() - pd.Timedelta(&quot;1 day&quot;)%%00010else:%%00010    last_query_day = pd.to_datetime(LAST_QUERY_DAY)%%00010%%00010# For daily data get last 7 days, no regard for day of week%%00010# For weekly data get Monday to Sunday data of last complete week%%00010start_dates = pd.date_range(%%00010    DATE_FIRST_EMPTY_DAY, last_query_day, freq=PERIOD_FREQ, closed=None%%00010)%%00010%%00010if not LINES_PER_REQUEST:%%00010    # Batchwise: get daily data %%00010    start_dates = [%%00010        date for date in start_dates if (date + pd.Timedelta(&quot;6 day&quot;)) &lt;= last_query_day%%00010    ]%%00010    %%00010end_dates = [%%00010    (date + pd.Timedelta(&quot;6 day&quot;)).strftime(&quot;%Y-%m-%d&quot;) for date in start_dates%%00010]%%00010start_dates = [date.strftime(&quot;%Y-%m-%d&quot;) for date in start_dates]%%00010%%00010# For monthly data get full last month%%00010if GRANULARITY == &quot;month&quot;:%%00010    start_dates = [%%00010        (%%00010            datetime.datetime.strptime(date, &quot;%Y-%m-%d&quot;).replace(day=1)%%00010            + relativedelta(months=0)%%00010        )%%00010        for date in start_dates%%00010    ]%%00010    end_dates = [%%00010        str((date + relativedelta(months=1, days=-1)).date()) for date in start_dates%%00010    ]%%00010    start_dates = [str(date.date()) for date in start_dates]%%00010%%00010# For testing only do a few entries%%00010if TEST == 1:%%00010    start_dates = start_dates[0:2]%%00010    end_dates = end_dates[0:2]%%00010    %%00010# For pagination query get day by day instead of weekly, get num of results%%00010if LINES_PER_REQUEST:%%00010    end_dates = [None for x in start_dates]    %%00010%%00010num_results = get_request_totals(start_dates, end_dates)%%00010queries = make_queries(start_dates, end_dates, num_results)%%00010df = batchwise_request(queries)%%00010%%00010# To Dataframe%%00010clean_data = df.drop_duplicates(keep=&quot;first&quot;)%%00010%%00010if KNIME:%%00010    # Bug in Knime when exporting: Related to DateTime%%00010    clean_data[&quot;load_timestamp&quot;] = clean_data[&quot;load_timestamp&quot;].astype(str)%%00010%%00010clean_data = clean_data.apply(lambda x: pd.to_numeric(x, errors=&quot;ignore&quot;))%%00010%%00010# Clean Chars%%00010clean_data = clean_data.applymap(%%00010    lambda x: x.encode(&quot;iso8859-15&quot;, &quot;replace&quot;).decode(&quot;iso8859-15&quot;)%%00010    if isinstance(x, str)%%00010    else x%%00010)%%00010output_table = clean_data.reset_index(drop=True)%%00010%%00010 "/>
<entry key="rowLimit" type="xint" value="1000"/>
<entry key="pythonVersionOption" type="xstring" value="python3"/>
<entry key="python2Command" type="xstring" value=""/>
<entry key="python3Command" type="xstring" value=""/>
<entry key="chunkSize" type="xint" value="500000"/>
<entry key="convertMissingToPython" type="xboolean" value="false"/>
<entry key="convertMissingFromPython" type="xboolean" value="false"/>
<entry key="sentinelOption" type="xstring" value="MIN_VAL"/>
<entry key="sentinelValue" type="xint" value="0"/>
</config>
<config key="nodeAnnotation">
<entry key="text" type="xstring" value="Request Adobe%%00013%%00010Report from API"/>
<entry key="bgcolor" type="xint" value="16777215"/>
<entry key="x-coordinate" type="xint" value="227"/>
<entry key="y-coordinate" type="xint" value="359"/>
<entry key="width" type="xint" value="106"/>
<entry key="height" type="xint" value="30"/>
<entry key="alignment" type="xstring" value="CENTER"/>
<entry key="borderSize" type="xint" value="0"/>
<entry key="borderColor" type="xint" value="16777215"/>
<entry key="defFontSize" type="xint" value="9"/>
<entry key="annotation-version" type="xint" value="20151123"/>
<config key="styles"/>
</config>
<entry key="customDescription" type="xstring" isnull="true" value=""/>
<entry key="state" type="xstring" value="EXECUTED"/>
<entry key="factory" type="xstring" value="org.knime.python2.nodes.source.Python2SourceNodeFactory"/>
<entry key="node-name" type="xstring" value="Python Source"/>
<entry key="node-bundle-name" type="xstring" value="KNIME Python nodes"/>
<entry key="node-bundle-symbolic-name" type="xstring" value="org.knime.python2.nodes"/>
<entry key="node-bundle-vendor" type="xstring" value="KNIME AG, Zurich, Switzerland"/>
<entry key="node-bundle-version" type="xstring" value="4.1.3.v202005112253"/>
<entry key="node-feature-name" type="xstring" value="KNIME Python Integration"/>
<entry key="node-feature-symbolic-name" type="xstring" value="org.knime.features.python2.feature.group"/>
<entry key="node-feature-vendor" type="xstring" value="KNIME AG, Zurich, Switzerland"/>
<entry key="node-feature-version" type="xstring" value="4.1.3.v202005112253"/>
<config key="factory_settings"/>
<entry key="name" type="xstring" value="Python Source"/>
<entry key="hasContent" type="xboolean" value="true"/>
<entry key="isInactive" type="xboolean" value="false"/>
<config key="ports">
<config key="port_1">
<entry key="index" type="xint" value="1"/>
<entry key="port_spec_class" type="xstring" value="org.knime.core.data.DataTableSpec"/>
<entry key="port_object_class" type="xstring" value="org.knime.core.node.BufferedDataTable"/>
<entry key="port_object_summary" type="xstring" value="Rows: 14607, Cols: 6"/>
<entry key="port_dir_location" type="xstring" value="port_1"/>
</config>
</config>
<config key="filestores">
<entry key="file_store_location" type="xstring" isnull="true" value=""/>
<entry key="file_store_id" type="xstring" value="58ee769c-ce99-4ab3-844d-4a068affeef7"/>
</config>
</config>
