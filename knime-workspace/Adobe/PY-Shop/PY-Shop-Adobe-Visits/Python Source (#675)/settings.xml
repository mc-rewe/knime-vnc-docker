<?xml version="1.0" encoding="UTF-8"?>
<config xmlns="http://www.knime.org/2008/09/XMLConfig" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.knime.org/2008/09/XMLConfig http://www.knime.org/XMLConfig_2008_09.xsd" key="settings.xml">
    <entry key="node_file" type="xstring" value="settings.xml"/>
    <config key="flow_stack"/>
    <config key="internal_node_subsettings">
        <entry key="memory_policy" type="xstring" value="CacheSmallInMemory"/>
    </config>
    <config key="model">
        <entry key="sourceCode" type="xstring" value="# coding: utf-8%%00010&quot;&quot;&quot; 2020-007-30 MC:  Adobe API 1.4 Functions %%00010&quot;&quot;&quot;%%00010import os%%00010import requests%%00010import json%%00010import datetime%%00010import time%%00010import uuid%%00010import binascii%%00010import hashlib%%00010import logging%%00010import sys%%00010import pandas as pd%%00010import numpy as np%%00010from math import ceil%%00010from dateutil.relativedelta import relativedelta%%00010%%00010# Logger setup%%00010log = logging.getLogger()%%00010log.addHandler(logging.StreamHandler())%%00010log.setLevel(logging.INFO)%%00010%%00010# For use in Knime workflow (True) or direct run (False)%%00010try:%%00010    log.debug(flow_variables)%%00010except Exception as e:%%00010    # Not in Knime%%00010    log.info(&quot;Not in Knime&quot;)%%00010    KNIME = False%%00010else:%%00010    log.info(&quot;Running in Knime&quot;)%%00010    KNIME = True%%00010    %%00010# Frist day to receive data for%%00010MANUAL_FIRST_EMPTY_DAY =  None  # Set to None%%00010%%00010if KNIME:%%00010    USER = flow_variables[&quot;ADOBE_USER&quot;]%%00010    PW = flow_variables[&quot;ADOBE_PW&quot;]%%00010    PROXY = flow_variables.get(&quot;PROXY&quot;)%%00010    # If we dont have data yet (setup of flow) manually set first day    %%00010    if MANUAL_FIRST_EMPTY_DAY:%%00010        DATE_FIRST_EMPTY_DAY = MANUAL_FIRST_EMPTY_DAY%%00010    else:%%00010        DATE_FIRST_EMPTY_DAY = flow_variables.get(&quot;last_entry&quot;)%%00010    TEST = flow_variables.get(&quot;TEST&quot;, 0)  # Set 0 for production%%00010else:%%00010    # Set those in your environment variables%%00010    USER = os.environ.get(&quot;ADOBE_USER&quot;)%%00010    PW = os.environ.get(&quot;ADOBE_PW&quot;)%%00010    PROXY = None%%00010    DATE_FIRST_EMPTY_DAY = MANUAL_FIRST_EMPTY_DAY%%00010%%00010PROXIES = None%%00010if PROXY is not None and PROXY != &quot;None&quot;:%%00010    PROXIES = {&quot;https&quot;: f&quot;{PROXY}&quot;}%%00010%%00010%%00010%%00010# PARAMETERS%%00010REPORT_SUITE = &quot;vrs_rewedi1_vrspennyshop&quot;%%00010API_URL = &quot;https://api.omniture.com/admin/1.4/rest/&quot;%%00010QUEUE_TIMEOUT = 3%%00010QUEUE_MAX_WAIT_TIME = 120%%00010%%00010log.info(f&quot;Proxyy: {PROXIES}\n&quot;)%%00010log.info(f&quot;First empty day: {DATE_FIRST_EMPTY_DAY}\n&quot;)%%00010%%00010TEST = 0%%00010# Granularity &quot;day&quot; or week%%00010GRANULARITY = &quot;day&quot;%%00010%%00010PERIOD_FREQ = &quot;7D&quot;%%00010if GRANULARITY == &quot;week&quot;:%%00010    PERIOD_FREQ = &quot;W-MON&quot;%%00010%%00010if not DATE_FIRST_EMPTY_DAY and TEST==1:%%00010    DATE_FIRST_EMPTY_DAY = &quot;2019-04-22&quot;%%00010# borrowed from adobe_analytics package%%00010def parse(raw_response):%%00010    log.info(&quot;Parsing raw json response.&quot;)%%00010    report = raw_response[&quot;report&quot;]%%00010    raw_data = report[&quot;data&quot;]%%00010%%00010    dimensions, metrics = _parse_header(report)%%00010    data = _parse_data(raw_data, metric_count=len(metrics))%%00010    header = _fix_header(dimensions, metrics, data)%%00010    return pd.DataFrame(data, columns=header)%%00010%%00010def _parse_header(report):%%00010    log.debug(&quot;Parsing dimensions and metrics.&quot;)%%00010    dimensions = [_classification_or_name(dimension) for dimension in report[&quot;elements&quot;]]%%00010    metrics = [metric[&quot;name&quot;] for metric in report[&quot;metrics&quot;]]%%00010    return dimensions, metrics%%00010%%00010def _classification_or_name(element):%%00010    if &quot;classification&quot; in element:%%00010        return element[&quot;classification&quot;]%%00010    return element[&quot;name&quot;]%%00010%%00010def _parse_data(data, metric_count):%%00010    &quot;&quot;&quot;%%00010    Recursive parsing of the &quot;data&quot; part of the Adobe response.%%00010    :param data: list of dicts and lists. quite a complicated structure%%00010    :param metric_count: int, number of metrics in report%%00010    :return: list of lists%%00010    &quot;&quot;&quot;%%00010    log.debug(&quot;Parsing report data (recursively).&quot;)%%00010    if len(data) &gt; 0 and &quot;breakdown&quot; in data[0]:%%00010        rows = list()%%00010        for chunk in data:%%00010            dim_value = _dimension_value(chunk)%%00010            rows += [[dim_value] + row%%00010                     for row in _parse_data(chunk[&quot;breakdown&quot;], metric_count)]%%00010        return rows%%00010    else:%%00010        return _parse_most_granular(data, metric_count)%%00010%%00010def _parse_most_granular(data, metric_count):%%00010    &quot;&quot;&quot;%%00010    Parsing of the most granular part of the response.%%00010    It is different depending on if there's a granularity breakdown or not%%00010    :param data: dict%%00010    :param metric_count: int, number of metrics in report%%00010    :return: list of lists%%00010    &quot;&quot;&quot;%%00010    log.debug(&quot;Parsing most granular level of data.&quot;)%%00010    rows = list()%%00010    for chunk in data:%%00010        part_rows = [(val if val != &quot;&quot; else np.nan) for val in chunk[&quot;counts&quot;]]%%00010        # data alignment is a bit different if adding granularity breakdowns%%00010        if len(chunk[&quot;counts&quot;]) &gt; metric_count:%%00010            part_rows = more_itertools.chunked(iterable=part_rows, n=metric_count + 1)%%00010        else:%%00010            part_rows = [part_rows]%%00010%%00010        dim_value = _dimension_value(chunk)%%00010        rows += [[dim_value] + part_row for part_row in part_rows]%%00010    return rows%%00010%%00010def _dimension_value(chunk):%%00010    if _dimension_value_is_nan(chunk):%%00010        return np.nan%%00010    elif &quot;year&quot; in chunk:%%00010        return _to_datetime(chunk)%%00010    else:%%00010        return chunk[&quot;name&quot;]%%00010%%00010def _dimension_value_is_nan(chunk):%%00010    return (&quot;name&quot; not in chunk) or (chunk[&quot;name&quot;] == &quot;&quot;) or (chunk[&quot;name&quot;] == &quot;::unspecified::&quot;)%%00010%%00010def _to_datetime(chunk):%%00010    time_stamp = datetime.datetime(%%00010        year=chunk[&quot;year&quot;],%%00010        month=chunk[&quot;month&quot;],%%00010        day=chunk[&quot;day&quot;],%%00010        hour=chunk.get(&quot;hour&quot;, 0)%%00010    )%%00010    return time_stamp.strftime(&quot;%Y-%m-%d %H:00:00&quot;)%%00010%%00010def _fix_header(dimensions, metrics, data):%%00010    header = dimensions + metrics%%00010    if len(header) != len(data[0]):  # can only be when granularity breakdown is used%%00010        return [&quot;Datetime&quot;] + header%%00010    return header%%00010%%00010def build_headers():%%00010    &quot;&quot;&quot; Create WSSE compliant headers for auth -&gt; see api doc%%00010    &quot;&quot;&quot;%%00010    nonce = str(uuid.uuid4())%%00010    base64nonce = binascii.b2a_base64(binascii.a2b_qp(nonce))%%00010    created_date = datetime.datetime.utcnow().isoformat()+'Z'%%00010    sha = nonce + created_date + PW%%00010    sha_object = hashlib.sha1(sha.encode())%%00010    password_64 = binascii.b2a_base64(sha_object.digest())%%00010%%00010    properties = {%%00010        &quot;Username&quot;: USER,%%00010        &quot;PasswordDigest&quot;: password_64.decode().strip(),%%00010        &quot;Nonce&quot;: base64nonce.decode().strip(),%%00010        &quot;Created&quot;: created_date%%00010    }%%00010    %%00010    header = ['{key}=&quot;{value}&quot;'.format(key=k, value=v) for k, v in properties.items()]%%00010    header = 'UsernameToken ' + ', '.join(header)%%00010    return {'X-WSSE': header}%%00010%%00010%%00010def split_ecid(data):%%00010    # Split ECID to its parts (should have max 9 parts) and truncate max length%%00010    if &quot;ecid&quot; in data.columns:    %%00010        ecid_split = data['ecid'].str.split('_', n=8, expand=True)%%00010        ecid_split = ecid_split.apply(lambda x: x.str.slice(0, 120))%%00010        ecid_split.columns = ['ecid_' + str(x+1) for x in range(len(ecid_split.columns))]%%00010        data = data.merge(ecid_split, left_index=True, right_index=True)%%00010        data['load_timestamp'] = datetime.datetime.now()    %%00010    return data    %%00010%%00010%%00010def queue_report(date_from, date_to):%%00010    &quot;&quot;&quot; Call Report.Queue method and return the reportID %%00010    &quot;&quot;&quot;%%00010    header = build_headers()%%00010    try:%%00010        r = requests.post(API_URL,%%00010                     params={'method': 'Report.Queue'},%%00010                     data=json.dumps({'reportDescription': {%%00010                         'reportSuiteID': REPORT_SUITE,%%00010                         'elements': [%%00010                                     ],                         %%00010                         'metrics': [%%00010                                     {'id': 'visits'}%%00010                                    ],                         %%00010                         &quot;segments&quot;: [{%%00010                                        &quot;element&quot;: &quot;page&quot;,%%00010                                        &quot;selected&quot;: [&quot;penny-de-shop:checkout:cart&quot;] %%00010                                     }],                         %%00010                         'dateFrom': date_from,%%00010                         'dateTo': date_to,%%00010                         'dateGranularity': 'day'}}),%%00010                      headers=header,%%00010                      proxies=PROXIES)%%00010    except Exception as e:%%00010        log.error(&quot;Could not post request: {}&quot;.format(e))%%00010        return None%%00010    else:%%00010        if r.status_code == 200:%%00010            report_id = r.json().get('reportID')%%00010            log.info(&quot;Successfully sent to queue. ReportID: {}. From {} to {}&quot;\%%00010                     .format(report_id, date_from, date_to))%%00010            return report_id%%00010        else:%%00010            log.error('Invalid response. Status: {}. Response {}'.format(r.status_code, r.text))%%00010            return None  %%00010%%00010%%00010def get_report(reportID, i = 0):%%00010    &quot;&quot;&quot; Call Report.Get method and return the report data %%00010    &quot;&quot;&quot;%%00010    header = build_headers()%%00010    try:%%00010        r = requests.post(API_URL,%%00010                     params={'method': 'Report.Get'},%%00010                     data=json.dumps({'reportID': '{}'.format(reportID), 'page': 1}),%%00010                     headers=header,%%00010                     proxies=PROXIES)%%00010    except Exception as e:%%00010        log.error(&quot;Could not post request: {}&quot;.format(e))%%00010        return None%%00010    else:%%00010        if r.status_code == 200:%%00010            log.info(&quot;Report returned successfully.&quot;)%%00010            return r.json()    %%00010        elif r.json().get('error') == &quot;report_not_ready&quot;:%%00010            time_wait = QUEUE_TIMEOUT * i%%00010            if time_wait &lt;= QUEUE_MAX_WAIT_TIME:%%00010                log.warning(&quot;Report not ready. Retrying in {} s&quot;.format(time_wait))%%00010                time.sleep(time_wait)%%00010                return get_report(reportID, i+1)%%00010            else:%%00010                log.error(&quot;Queue wait too long. Report not retrieved.&quot;)%%00010                return None%%00010        else:%%00010            log.error('Invalid response. Status: {}. Response {}'.format(r.status_code, r.text))%%00010            return None         %%00010%%00010def data_to_df(report_data):%%00010    &quot;&quot;&quot; Parse and pre-process Data: Explode ECID Parts to columns%%00010        Convert DF to list and send to DB%%00010    &quot;&quot;&quot;%%00010    data = parse(report_data)%%00010    data.columns = ['date_day', 'visits']%%00010    data = data[data['date_day'].notna()]    %%00010    data = data.drop_duplicates(['date_day', 'visits'], keep='last')%%00010    data = split_ecid(data)%%00010    return data%%00010%%00010#date_range = pd.date_range('2019-07-08', '2019-07-20').strftime('%Y-%m-%d').to_list()%%00010report_list = []%%00010clean_data = pd.DataFrame()%%00010yesterday = pd.datetime.today() - pd.Timedelta('1 day')%%00010%%00010if DATE_FIRST_EMPTY_DAY &gt;= yesterday.strftime(&quot;%Y-%m-%d&quot;):%%00010    # No new data yet - exit%%00010    log.warning(%%00010        f&quot;DATE_FIRST_EMPTY_DAY: {DATE_FIRST_EMPTY_DAY} &gt;= yesterday: {yesterday}&quot;%%00010    )%%00010    sys.exit(0)%%00010%%00010date_range = pd.date_range(DATE_FIRST_EMPTY_DAY, yesterday).strftime('%Y-%m-%d').tolist()%%00010%%00010&quot;&quot;&quot; Send all Reports to queue%%00010&quot;&quot;&quot;    %%00010report_id = queue_report(date_range[0], date_range[-1])%%00010        %%00010&quot;&quot;&quot; Retrieve all reports and process them to df%%00010&quot;&quot;&quot;%%00010report_data = get_report(report_id)%%00010data_df = data_to_df(report_data)%%00010clean_data = clean_data.append(data_df)%%00010%%00010# To Knime - Clean Chars and duplicates%%00010clean_data = clean_data.applymap(lambda x:%%00010                         x.encode('iso8859-15', 'replace')\%%00010                         .decode('iso8859-15') if isinstance(x, str) else x)%%00010clean_data = clean_data.drop_duplicates(['date_day', 'visits'], keep='last')%%00010%%00010%%00010# Bug in Knime when exporting: Related to DateTime%%00010clean_data['date_day'] = clean_data['date_day'].astype(str)%%00010output_table = clean_data.reset_index(drop=True)%%00010%%00010"/>
        <entry key="rowLimit" type="xint" value="1000"/>
        <entry key="convertMissingToPython" type="xboolean" value="false"/>
        <entry key="convertMissingFromPython" type="xboolean" value="false"/>
        <entry key="sentinelOption" type="xstring" value="MIN_VAL"/>
        <entry key="sentinelValue" type="xint" value="0"/>
        <entry key="chunkSize" type="xint" value="500000"/>
        <entry key="pythonVersionOption" type="xstring" value="python3"/>
        <entry key="python2Command" type="xstring" value=""/>
        <entry key="python3Command" type="xstring" value=""/>
    </config>
    <config key="nodeAnnotation">
        <entry key="text" type="xstring" value="Request Adobe%%00013%%00010Report from API"/>
        <entry key="bgcolor" type="xint" value="16777215"/>
        <entry key="x-coordinate" type="xint" value="189"/>
        <entry key="y-coordinate" type="xint" value="299"/>
        <entry key="width" type="xint" value="142"/>
        <entry key="height" type="xint" value="34"/>
        <entry key="alignment" type="xstring" value="CENTER"/>
        <entry key="borderSize" type="xint" value="0"/>
        <entry key="borderColor" type="xint" value="16777215"/>
        <entry key="defFontSize" type="xint" value="9"/>
        <entry key="annotation-version" type="xint" value="20151123"/>
        <config key="styles"/>
    </config>
    <entry key="customDescription" type="xstring" isnull="true" value=""/>
    <entry key="state" type="xstring" value="EXECUTED"/>
    <entry key="factory" type="xstring" value="org.knime.python2.nodes.source.Python2SourceNodeFactory"/>
    <entry key="node-name" type="xstring" value="Python Source"/>
    <entry key="node-bundle-name" type="xstring" value="KNIME Python nodes"/>
    <entry key="node-bundle-symbolic-name" type="xstring" value="org.knime.python2.nodes"/>
    <entry key="node-bundle-vendor" type="xstring" value="KNIME AG, Zurich, Switzerland"/>
    <entry key="node-bundle-version" type="xstring" value="4.4.2.v202110181015"/>
    <entry key="node-feature-name" type="xstring" value="KNIME Python Integration"/>
    <entry key="node-feature-symbolic-name" type="xstring" value="org.knime.features.python2.feature.group"/>
    <entry key="node-feature-vendor" type="xstring" value="KNIME AG, Zurich, Switzerland"/>
    <entry key="node-feature-version" type="xstring" value="4.4.2.v202110191522"/>
    <config key="factory_settings"/>
    <entry key="name" type="xstring" value="Python Source"/>
    <entry key="hasContent" type="xboolean" value="true"/>
    <entry key="isInactive" type="xboolean" value="false"/>
    <config key="ports">
        <config key="port_1">
            <entry key="index" type="xint" value="1"/>
            <entry key="port_spec_class" type="xstring" value="org.knime.core.data.DataTableSpec"/>
            <entry key="port_object_class" type="xstring" value="org.knime.core.node.BufferedDataTable"/>
            <entry key="port_object_summary" type="xstring" value="Rows: 5, Cols: 2"/>
            <entry key="port_dir_location" type="xstring" value="port_1"/>
        </config>
    </config>
    <config key="filestores">
        <entry key="file_store_location" type="xstring" isnull="true" value=""/>
        <entry key="file_store_id" type="xstring" value="a2b590a8-4038-4cc1-9f43-5d533e5551de"/>
    </config>
</config>
