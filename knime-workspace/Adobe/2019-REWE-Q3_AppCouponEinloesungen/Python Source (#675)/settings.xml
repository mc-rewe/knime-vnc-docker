<?xml version="1.0" encoding="UTF-8"?>
<config xmlns="http://www.knime.org/2008/09/XMLConfig" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.knime.org/2008/09/XMLConfig http://www.knime.org/XMLConfig_2008_09.xsd" key="settings.xml">
<entry key="node_file" type="xstring" value="settings.xml"/>
<config key="flow_stack"/>
<config key="internal_node_subsettings">
<entry key="memory_policy" type="xstring" value="CacheSmallInMemory"/>
</config>
<config key="model">
<entry key="sourceCode" type="xstring" value="# coding: utf-8%%00010&quot;&quot;&quot; 2019-06-27 MC:  Adobe API 1.4 Functions %%00010    Mostly from adobe_analytics python package%%00010    Get Report Data, process it, write it to TeraData DB%%00010&quot;&quot;&quot;%%00010import os%%00010import requests%%00010import json%%00010import datetime%%00010import time%%00010import uuid%%00010import binascii%%00010import hashlib%%00010import logging%%00010import pandas as pd%%00010import numpy as np%%00010import pyodbc as pdb%%00010from math import ceil%%00010%%00010# Logger setup%%00010log = logging.getLogger()%%00010log.addHandler(logging.StreamHandler())%%00010log.setLevel(logging.DEBUG)%%00010%%00010a1 = flow_variables.get('a1')%%00010a2 = flow_variables.get('a2')%%00010%%00010USER = flow_variables['ADOBE_USER']%%00010PW = flow_variables['ADOBE_PW']%%00010%%00010DATE_LAST_ENTRY = flow_variables.get('last_entry')%%00010PROXIES = {&quot;https&quot; : &quot;https://{}:{}@risproxy.risnet.de:80&quot;.format(a1, a2)}%%00010%%00010# PARAMETERS%%00010API_URL = 'https://api.omniture.com/admin/1.4/rest/'%%00010QUEUE_TIMEOUT = 3%%00010QUEUE_MAX_WAIT_TIME = 120%%00010# SQL %%00010SQL_TABLE = &quot;CIA_MC_REWE_ADOBE_2019Q3_APP_BENEFITS_UNIQUEVISITORS&quot;%%00010ROWS_PER_INSERT = 1000%%00010%%00010%%00010def parse(raw_response):%%00010    log.info(&quot;Parsing raw json response.&quot;)%%00010    report = raw_response[&quot;report&quot;]%%00010    raw_data = report[&quot;data&quot;]%%00010%%00010    dimensions, metrics = _parse_header(report)%%00010    data = _parse_data(raw_data, metric_count=len(metrics))%%00010    header = _fix_header(dimensions, metrics, data)%%00010    return pd.DataFrame(data, columns=header)%%00010%%00010%%00010def _parse_header(report):%%00010    log.debug(&quot;Parsing dimensions and metrics.&quot;)%%00010    dimensions = [_classification_or_name(dimension) for dimension in report[&quot;elements&quot;]]%%00010    metrics = [metric[&quot;name&quot;] for metric in report[&quot;metrics&quot;]]%%00010    return dimensions, metrics%%00010%%00010%%00010def _classification_or_name(element):%%00010    if &quot;classification&quot; in element:%%00010        return element[&quot;classification&quot;]%%00010    return element[&quot;name&quot;]%%00010%%00010def _parse_data(data, metric_count):%%00010    &quot;&quot;&quot;%%00010    Recursive parsing of the &quot;data&quot; part of the Adobe response.%%00010    :param data: list of dicts and lists. quite a complicated structure%%00010    :param metric_count: int, number of metrics in report%%00010    :return: list of lists%%00010    &quot;&quot;&quot;%%00010    log.debug(&quot;Parsing report data (recursively).&quot;)%%00010    if len(data) &gt; 0 and &quot;breakdown&quot; in data[0]:%%00010        rows = list()%%00010        for chunk in data:%%00010            dim_value = _dimension_value(chunk)%%00010            rows += [[dim_value] + row%%00010                     for row in _parse_data(chunk[&quot;breakdown&quot;], metric_count)]%%00010        return rows%%00010    else:%%00010        return _parse_most_granular(data, metric_count)%%00010%%00010%%00010def _parse_most_granular(data, metric_count):%%00010    &quot;&quot;&quot;%%00010    Parsing of the most granular part of the response.%%00010    It is different depending on if there's a granularity breakdown or not%%00010    :param data: dict%%00010    :param metric_count: int, number of metrics in report%%00010    :return: list of lists%%00010    &quot;&quot;&quot;%%00010    log.debug(&quot;Parsing most granular level of data.&quot;)%%00010    rows = list()%%00010    for chunk in data:%%00010        part_rows = [(val if val != &quot;&quot; else np.nan) for val in chunk[&quot;counts&quot;]]%%00010        # data alignment is a bit different if adding granularity breakdowns%%00010        if len(chunk[&quot;counts&quot;]) &gt; metric_count:%%00010            part_rows = more_itertools.chunked(iterable=part_rows, n=metric_count + 1)%%00010        else:%%00010            part_rows = [part_rows]%%00010%%00010        dim_value = _dimension_value(chunk)%%00010        rows += [[dim_value] + part_row for part_row in part_rows]%%00010    return rows%%00010%%00010%%00010def _dimension_value(chunk):%%00010    if _dimension_value_is_nan(chunk):%%00010        return np.nan%%00010    elif &quot;year&quot; in chunk:%%00010        return _to_datetime(chunk)%%00010    else:%%00010        return chunk[&quot;name&quot;]%%00010%%00010%%00010def _dimension_value_is_nan(chunk):%%00010    return (&quot;name&quot; not in chunk) or (chunk[&quot;name&quot;] == &quot;&quot;) or (chunk[&quot;name&quot;] == &quot;::unspecified::&quot;)%%00010%%00010%%00010def _to_datetime(chunk):%%00010    time_stamp = datetime.datetime(%%00010        year=chunk[&quot;year&quot;],%%00010        month=chunk[&quot;month&quot;],%%00010        day=chunk[&quot;day&quot;],%%00010        hour=chunk.get(&quot;hour&quot;, 0)%%00010    )%%00010    return time_stamp.strftime(&quot;%Y-%m-%d %H:00:00&quot;)%%00010%%00010%%00010def _fix_header(dimensions, metrics, data):%%00010    header = dimensions + metrics%%00010    if len(header) != len(data[0]):  # can only be when granularity breakdown is used%%00010        return [&quot;Datetime&quot;] + header%%00010    return header%%00010%%00010%%00010def build_headers():%%00010    &quot;&quot;&quot; Create WSSE compliant headers for auth -&gt; see api doc%%00010    &quot;&quot;&quot;%%00010    nonce = str(uuid.uuid4())%%00010    base64nonce = binascii.b2a_base64(binascii.a2b_qp(nonce))%%00010    created_date = datetime.datetime.utcnow().isoformat()+'Z'%%00010    sha = nonce + created_date + PW%%00010    sha_object = hashlib.sha1(sha.encode())%%00010    password_64 = binascii.b2a_base64(sha_object.digest())%%00010%%00010    properties = {%%00010        &quot;Username&quot;: USER,%%00010        &quot;PasswordDigest&quot;: password_64.decode().strip(),%%00010        &quot;Nonce&quot;: base64nonce.decode().strip(),%%00010        &quot;Created&quot;: created_date%%00010    }%%00010    %%00010    header = ['{key}=&quot;{value}&quot;'.format(key=k, value=v) for k, v in properties.items()]%%00010    header = 'UsernameToken ' + ', '.join(header)%%00010    return {'X-WSSE': header}%%00010%%00010%%00010def queue_report(date_from, date_to):%%00010    &quot;&quot;&quot; Call Report.Queue method and return the reportID %%00010    &quot;&quot;&quot;%%00010    header = build_headers()%%00010    try:%%00010        r = requests.post(API_URL,%%00010                     params={'method': 'Report.Queue'},%%00010                     data=json.dumps({'reportDescription': {%%00010                         'reportSuiteID': 'rewappprod',%%00010                         'elements': [{'id': 'evar230', 'top': 50000}],%%00010                         'metrics': [{'id': 'uniquevisitors'}],%%00010                         'dateFrom': date_from,%%00010                         'dateTo': date_to,%%00010                         'dateGranularity': 'day'%%00010                      }}),%%00010                      headers=header)%%00010    except Exception as e:%%00010        log.error(&quot;Could not post request: {}&quot;.format(e))%%00010        return None%%00010    else:%%00010        if r.status_code == 200:%%00010            report_id = r.json().get('reportID')%%00010            log.info(&quot;Successfully sent to queue. ReportID: {}. From {} to {}&quot;.format(report_id, date_from, date_to))%%00010            return report_id%%00010        else:%%00010            log.error('Invalid response. Status: {}. Response {}'.format(r.status_code, r.text))%%00010            return None    %%00010%%00010%%00010def get_report(reportID, i = 0):%%00010    &quot;&quot;&quot; Call Report.Get method and return the report data %%00010    &quot;&quot;&quot;%%00010    header = build_headers()%%00010    try:%%00010        r = requests.post(API_URL,%%00010                     params={'method': 'Report.Get'},%%00010                     data=json.dumps({'reportID': '{}'.format(reportID), 'page': 1}),%%00010                     headers=header)%%00010    except Exception as e:%%00010        log.error(&quot;Could not post request: {}&quot;.format(e))%%00010        return None%%00010    else:%%00010        if r.status_code == 200:%%00010            log.info(&quot;Report returned successfully.&quot;)%%00010            return r.json()    %%00010        elif r.json().get('error') == &quot;report_not_ready&quot;:%%00010            time_wait = QUEUE_TIMEOUT * i%%00010            if time_wait &lt;= QUEUE_MAX_WAIT_TIME:%%00010                log.warning(&quot;Report not ready. Retrying in {} s&quot;.format(time_wait))%%00010                time.sleep(time_wait)%%00010                return get_report(reportID, i+1)%%00010            else:%%00010                log.error(&quot;Queue wait too long. Report not retrieved.&quot;)%%00010                return None%%00010        else:%%00010            log.error('Invalid response. Status: {}. Response {}'.format(r.status_code, r.text))%%00010            return None        %%00010%%00010%%00010def data_to_df(report_data, order_day):%%00010    &quot;&quot;&quot; Parse and pre-process Data: Explode ECID Parts to columns%%00010        Convert DF to list and send to DB%%00010    &quot;&quot;&quot;%%00010    data = parse(report_data)%%00010    data.columns= ['date_time', 'benefit','uniquevisitors']%%00010    data = data[data['benefit'].notna()]    %%00010    data = data.drop_duplicates(['benefit', 'date_time', 'uniquevisitors'], keep='last')%%00010    data['order_date'] = order_day%%00010    # Split ECID to its parts (should have max 9 parts) and truncate max length%%00010    ecid_split = data['benefit'].str.split('_', n=4, expand=True)%%00010    ecid_split = ecid_split.apply(lambda x: x.str.slice(0, 120))%%00010    ecid_split.columns = ['benefit' + str(x+1) for x in range(len(ecid_split.columns))]%%00010    data = data.merge(ecid_split, left_index=True, right_index=True)%%00010    data['load_timestamp'] = datetime.datetime.now()    %%00010    return data   %%00010%%00010%%00010%%00010#################################################################################%%00010# Main Part %%00010#################################################################################%%00010report_list = []%%00010clean_data = pd.DataFrame()%%00010yesterday = pd.datetime.today() - pd.Timedelta('1 day')%%00010%%00010date_range = pd.date_range(DATE_LAST_ENTRY, yesterday).strftime('%Y-%m-%d').tolist()%%00010%%00010for day in date_range:%%00010    &quot;&quot;&quot; Send all Reports to queue%%00010    &quot;&quot;&quot;    %%00010    report_id = queue_report(day, day)%%00010    report_list.append((report_id, day))%%00010        %%00010for report in report_list:    %%00010    &quot;&quot;&quot; Retrieve all reports and process them to df%%00010    &quot;&quot;&quot;%%00010    report_data = get_report(report[0])%%00010    data_df = data_to_df(report_data, report[1])%%00010    clean_data = clean_data.append(data_df)%%00010%%00010# To Knime%%00010clean_data = clean_data.drop_duplicates(['benefit', 'date_time', 'uniquevisitors'], keep='last')%%00010# Bug in Knime when exporting: Related to DateTime%%00010clean_data['date_time'] = clean_data['date_time'].astype(str)%%00010clean_data['order_date'] = clean_data['order_date'].astype(str)%%00010clean_data['load_timestamp'] = clean_data['load_timestamp'].astype(str)%%00010output_table = clean_data.reset_index(drop=True)%%00010%%00010%%00010"/>
<entry key="rowLimit" type="xint" value="1000"/>
<entry key="pythonVersionOption" type="xstring" value="PYTHON3"/>
<entry key="convertMissingToPython" type="xboolean" value="false"/>
<entry key="convertMissingFromPython" type="xboolean" value="false"/>
<entry key="sentinelOption" type="xstring" value="MIN_VAL"/>
<entry key="sentinelValue" type="xint" value="0"/>
<entry key="chunkSize" type="xint" value="500000"/>
<entry key="python2Command" type="xstring" value=""/>
<entry key="python3Command" type="xstring" value=""/>
</config>
<config key="nodeAnnotation">
<entry key="text" type="xstring" value="Request Adobe%%00013%%00010Report from API"/>
<entry key="bgcolor" type="xint" value="16777215"/>
<entry key="x-coordinate" type="xint" value="147"/>
<entry key="y-coordinate" type="xint" value="299"/>
<entry key="width" type="xint" value="106"/>
<entry key="height" type="xint" value="30"/>
<entry key="alignment" type="xstring" value="CENTER"/>
<entry key="borderSize" type="xint" value="0"/>
<entry key="borderColor" type="xint" value="16777215"/>
<entry key="defFontSize" type="xint" value="9"/>
<entry key="annotation-version" type="xint" value="20151123"/>
<config key="styles"/>
</config>
<entry key="customDescription" type="xstring" isnull="true" value=""/>
<entry key="state" type="xstring" value="EXECUTED"/>
<entry key="factory" type="xstring" value="org.knime.python2.nodes.source.Python2SourceNodeFactory"/>
<entry key="node-name" type="xstring" value="Python Source"/>
<entry key="node-bundle-name" type="xstring" value="KNIME Python nodes"/>
<entry key="node-bundle-symbolic-name" type="xstring" value="org.knime.python2.nodes"/>
<entry key="node-bundle-vendor" type="xstring" value="KNIME AG, Zurich, Switzerland"/>
<entry key="node-bundle-version" type="xstring" value="3.7.2.v201904170931"/>
<entry key="node-feature-name" type="xstring" value="KNIME Python Integration"/>
<entry key="node-feature-symbolic-name" type="xstring" value="org.knime.features.python2.feature.group"/>
<entry key="node-feature-vendor" type="xstring" value="KNIME AG, Zurich, Switzerland"/>
<entry key="node-feature-version" type="xstring" value="3.7.2.v201904170931"/>
<config key="factory_settings"/>
<entry key="name" type="xstring" value="Python Source"/>
<entry key="hasContent" type="xboolean" value="true"/>
<entry key="isInactive" type="xboolean" value="false"/>
<config key="ports">
<config key="port_1">
<entry key="index" type="xint" value="1"/>
<entry key="port_spec_class" type="xstring" value="org.knime.core.data.DataTableSpec"/>
<entry key="port_object_class" type="xstring" value="org.knime.core.node.BufferedDataTable"/>
<entry key="port_object_summary" type="xstring" value="Rows: 44, Cols: 9"/>
<entry key="port_dir_location" type="xstring" value="port_1"/>
</config>
</config>
<config key="filestores">
<entry key="file_store_location" type="xstring" isnull="true" value=""/>
<entry key="file_store_id" type="xstring" value="c4db287a-24d5-4c06-b5e9-07760f665295"/>
</config>
</config>
