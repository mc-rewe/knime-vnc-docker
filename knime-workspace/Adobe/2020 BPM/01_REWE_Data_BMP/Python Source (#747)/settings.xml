<?xml version="1.0" encoding="UTF-8"?>
<config xmlns="http://www.knime.org/2008/09/XMLConfig" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.knime.org/2008/09/XMLConfig http://www.knime.org/XMLConfig_2008_09.xsd" key="settings.xml">
<entry key="node_file" type="xstring" value="settings.xml"/>
<config key="flow_stack"/>
<config key="internal_node_subsettings">
<entry key="memory_policy" type="xstring" value="CacheSmallInMemory"/>
</config>
<config key="model">
<entry key="sourceCode" type="xstring" value="# coding: utf-8%%00010&quot;&quot;&quot; 2020-007-30 MC:  Adobe API 1.4 Functions %%00010&quot;&quot;&quot;%%00010import os%%00010import requests%%00010import json%%00010import datetime%%00010import time%%00010import uuid%%00010import binascii%%00010import hashlib%%00010import logging%%00010import sys%%00010import tempfile%%00010import pandas as pd%%00010import numpy as np%%00010from math import ceil%%00010from dateutil.relativedelta import relativedelta%%00010%%00010# Logger setup%%00010log = logging.getLogger()%%00010log.addHandler(logging.StreamHandler())%%00010log.setLevel(logging.INFO)%%00010%%00010FILE_PATH = os.path.join(tempfile.gettempdir(), &quot;python_log.txt&quot;)%%00010%%00010# create file handler to temp file%%00010fh = logging.FileHandler(FILE_PATH)%%00010fh.setLevel(logging.DEBUG)%%00010log.addHandler(fh)%%00010%%00010%%00010# For use in Knime workflow (True) or direct run (False)%%00010try:%%00010%%00009log.debug(flow_variables)%%00010except Exception as e:%%00010%%00009# Not in Knime%%00010%%00009log.info(&quot;Not in Knime&quot;)%%00010%%00009KNIME = False%%00010else:%%00010%%00009log.info(&quot;Running in Knime&quot;)%%00010%%00009KNIME = True%%00010%%00009%%00010# Frist day to receive data for%%00010MANUAL_FIRST_EMPTY_DAY =  None  # Set to None%%00010%%00010if KNIME:%%00010    USER = flow_variables[&quot;ADOBE_USER&quot;]%%00010    PW = flow_variables[&quot;ADOBE_PW&quot;]%%00010    PROXY = flow_variables.get(&quot;PROXY&quot;)%%00010    # If we dont have data yet (setup of flow) manually set first day    %%00010    if MANUAL_FIRST_EMPTY_DAY:%%00010    %%00009DATE_FIRST_EMPTY_DAY = MANUAL_FIRST_EMPTY_DAY%%00010    else:%%00010    %%00009DATE_FIRST_EMPTY_DAY = flow_variables.get(&quot;last_entry&quot;)%%00010    TEST = flow_variables.get(&quot;TEST&quot;, 0)  # Set 0 for production%%00010else:%%00010    # Set those in your environment variables%%00010    USER = os.environ.get(&quot;ADOBE_USER&quot;)%%00010    PW = os.environ.get(&quot;ADOBE_PW&quot;)%%00010    PROXY = None%%00010    DATE_FIRST_EMPTY_DAY = MANUAL_FIRST_EMPTY_DAY%%00010%%00010PROXIES = None%%00010if PROXY is not None and PROXY != &quot;None&quot;:%%00010    PROXIES = {&quot;https&quot;: f&quot;{PROXY}&quot;}%%00010%%00010API_URL = &quot;https://api.omniture.com/admin/1.4/rest/&quot;%%00010QUEUE_TIMEOUT = 3%%00010QUEUE_MAX_WAIT_TIME = 120%%00010%%00010log.info(f&quot;Proxyy: {PROXIES}\n&quot;)%%00010log.info(f&quot;First empty day: {DATE_FIRST_EMPTY_DAY}\n&quot;)%%00010%%00010TEST = 0%%00010# Granularity &quot;day&quot; or week%%00010GRANULARITY = &quot;week&quot;%%00010%%00010PERIOD_FREQ = &quot;7D&quot;%%00010if GRANULARITY == &quot;week&quot;:%%00010    PERIOD_FREQ = &quot;W-MON&quot;%%00010%%00010if not DATE_FIRST_EMPTY_DAY and TEST==1:%%00010%%00009DATE_FIRST_EMPTY_DAY = &quot;2019-04-22&quot;%%00010%%00010def parse(raw_response):%%00010    log.info(&quot;Parsing raw json response.&quot;)%%00010    report = raw_response[&quot;report&quot;]%%00010    raw_data = report[&quot;data&quot;]%%00010%%00010    dimensions, metrics = _parse_header(report)%%00010    data = _parse_data(raw_data, metric_count=len(metrics))%%00010    header = _fix_header(dimensions, metrics, data)%%00010    return pd.DataFrame(data, columns=header)%%00010%%00010%%00010def _parse_header(report):%%00010    log.debug(&quot;Parsing dimensions and metrics.&quot;)%%00010    dimensions = [%%00010        _classification_or_name(dimension) for dimension in report[&quot;elements&quot;]%%00010    ]%%00010    metrics = [metric[&quot;name&quot;] for metric in report[&quot;metrics&quot;]]%%00010    return dimensions, metrics%%00010%%00010%%00010def _classification_or_name(element):%%00010    if &quot;classification&quot; in element:%%00010        return element[&quot;classification&quot;]%%00010    return element[&quot;name&quot;]%%00010%%00010%%00010def _parse_data(data, metric_count):%%00010    &quot;&quot;&quot;%%00010    Recursive parsing of the &quot;data&quot; part of the Adobe response.%%00010    :param data: list of dicts and lists. quite a complicated structure%%00010    :param metric_count: int, number of metrics in report%%00010    :return: list of lists%%00010    &quot;&quot;&quot;%%00010    log.debug(&quot;Parsing report data (recursively).&quot;)%%00010    if len(data) &gt; 0 and &quot;breakdown&quot; in data[0]:%%00010        rows = list()%%00010        for chunk in data:%%00010            dim_value = _dimension_value(chunk)%%00010            rows += [%%00010                [dim_value] + row%%00010                for row in _parse_data(chunk[&quot;breakdown&quot;], metric_count)%%00010            ]%%00010        return rows%%00010    else:%%00010        return _parse_most_granular(data, metric_count)%%00010%%00010%%00010def _parse_most_granular(data, metric_count):%%00010    &quot;&quot;&quot;%%00010    Parsing of the most granular part of the response.%%00010    It is different depending on if there's a granularity breakdown or not%%00010    :param data: dict%%00010    :param metric_count: int, number of metrics in report%%00010    :return: list of lists%%00010    &quot;&quot;&quot;%%00010    log.debug(&quot;Parsing most granular level of data.&quot;)%%00010    rows = list()%%00010    for chunk in data:%%00010        part_rows = [(val if val != &quot;&quot; else np.nan) for val in chunk[&quot;counts&quot;]]%%00010        # data alignment is a bit different if adding granularity breakdowns%%00010        if len(chunk[&quot;counts&quot;]) &gt; metric_count:%%00010            part_rows = more_itertools.chunked(iterable=part_rows, n=metric_count + 1)%%00010        else:%%00010            part_rows = [part_rows]%%00010%%00010        dim_value = _dimension_value(chunk)%%00010        rows += [[dim_value] + part_row for part_row in part_rows]%%00010    return rows%%00010%%00010%%00010def _dimension_value(chunk):%%00010    if _dimension_value_is_nan(chunk):%%00010        return np.nan%%00010    elif &quot;year&quot; in chunk:%%00010        return _to_datetime(chunk)%%00010    else:%%00010        return chunk[&quot;name&quot;]%%00010%%00010%%00010def _dimension_value_is_nan(chunk):%%00010    return (%%00010        (&quot;name&quot; not in chunk)%%00010        or (chunk[&quot;name&quot;] == &quot;&quot;)%%00010        or (chunk[&quot;name&quot;] == &quot;::unspecified::&quot;)%%00010    )%%00010%%00010%%00010def _to_datetime(chunk):%%00010    time_stamp = datetime.datetime(%%00010        year=chunk[&quot;year&quot;],%%00010        month=chunk[&quot;month&quot;],%%00010        day=chunk[&quot;day&quot;],%%00010        hour=chunk.get(&quot;hour&quot;, 0),%%00010    )%%00010    return time_stamp.strftime(&quot;%Y-%m-%d %H:00:00&quot;)%%00010%%00010%%00010def _fix_header(dimensions, metrics, data):%%00010    header = dimensions + metrics%%00010    if len(header) != len(data[0]):  # can only be when granularity breakdown is used%%00010        return [&quot;Datetime&quot;] + header%%00010    return header%%00010%%00010%%00010def build_headers():%%00010    &quot;&quot;&quot; Create WSSE compliant headers for auth -&gt; see api doc%%00010    &quot;&quot;&quot;%%00010    nonce = str(uuid.uuid4())%%00010    base64nonce = binascii.b2a_base64(binascii.a2b_qp(nonce))%%00010    created_date = datetime.datetime.utcnow().isoformat() + &quot;Z&quot;%%00010    sha = nonce + created_date + PW%%00010    sha_object = hashlib.sha1(sha.encode())%%00010    password_64 = binascii.b2a_base64(sha_object.digest())%%00010%%00010    properties = {%%00010        &quot;Username&quot;: USER,%%00010        &quot;PasswordDigest&quot;: password_64.decode().strip(),%%00010        &quot;Nonce&quot;: base64nonce.decode().strip(),%%00010        &quot;Created&quot;: created_date,%%00010    }%%00010%%00010    header = ['{key}=&quot;{value}&quot;'.format(key=k, value=v) for k, v in properties.items()]%%00010    header = &quot;UsernameToken &quot; + &quot;, &quot;.join(header)%%00010    return {&quot;X-WSSE&quot;: header}%%00010%%00010def queue_report(date_from, date_to):%%00010    &quot;&quot;&quot; Call Report.Queue method and return the reportID %%00010    &quot;&quot;&quot;%%00010    header = build_headers()%%00010    try:%%00010        r = requests.post(%%00010            API_URL,%%00010            params={&quot;method&quot;: &quot;Report.Queue&quot;},%%00010            data=json.dumps(%%00010                {%%00010                    &quot;reportDescription&quot;: {%%00010                        &quot;reportSuiteID&quot;: &quot;rewappprod&quot;,%%00010                        #&quot;elements&quot;: [{&quot;id&quot;: &quot;evar143&quot;, &quot;top&quot;: 100}],%%00010                        &quot;metrics&quot;: [{&quot;id&quot;: &quot;uniquevisitors&quot;}],%%00010                        &quot;segments&quot;: [{&quot;id&quot;: &quot;s830_5ebd57790aa5c36bec8e95dc&quot;}],%%00010                        &quot;dateGranularity&quot;: GRANULARITY,%%00010                        &quot;dateFrom&quot;: date_from,%%00010                        &quot;dateTo&quot;: date_to,%%00010                    }%%00010                }%%00010            ),%%00010            headers=header,%%00010            proxies=PROXIES,%%00010        )%%00010    except Exception as e:%%00010        log.error(&quot;Could not post request: {}&quot;.format(e))%%00010        return None%%00010    else:%%00010        if r.status_code == 200:%%00010            report_id = r.json().get(&quot;reportID&quot;)%%00010            log.info(%%00010                &quot;Successfully sent to queue. ReportID: {}. From {} to {}&quot;.format(%%00010                    report_id, date_from, date_to%%00010                )%%00010            )%%00010            return report_id%%00010        else:%%00010            log.error(%%00010                &quot;Invalid response. Status: {}. Response {}&quot;.format(%%00010                    r.status_code, r.text%%00010                )%%00010            )%%00010            return None%%00010%%00010%%00010def get_report(reportID, i=0):%%00010    &quot;&quot;&quot; Call Report.Get method and return the report data %%00010    &quot;&quot;&quot;%%00010    header = build_headers()%%00010    try:%%00010        r = requests.post(%%00010            API_URL,%%00010            params={&quot;method&quot;: &quot;Report.Get&quot;},%%00010            data=json.dumps({&quot;reportID&quot;: &quot;{}&quot;.format(reportID), &quot;page&quot;: 1}),%%00010            headers=header,%%00010            proxies=PROXIES,%%00010        )%%00010    except Exception as e:%%00010        log.error(&quot;Could not post request: {}&quot;.format(e))%%00010        return None%%00010    else:%%00010        if r.status_code == 200:%%00010            log.info(f&quot;Report returned successfully: {reportID}&quot;)%%00010            return r.json()%%00010        elif r.json().get(&quot;error&quot;) == &quot;report_not_ready&quot;:%%00010            time_wait = QUEUE_TIMEOUT * i%%00010            if time_wait &lt;= QUEUE_MAX_WAIT_TIME:%%00010                log.warning(&quot;Report not ready. Retrying in {} s&quot;.format(time_wait))%%00010                time.sleep(time_wait)%%00010                return get_report(reportID, i + 1)%%00010            else:%%00010                log.error(&quot;Queue wait too long. Report not retrieved.&quot;)%%00010                return None%%00010        else:%%00010            log.error(%%00010                &quot;Invalid response. Status: {}. Response {}&quot;.format(%%00010                    r.status_code, r.text%%00010                )%%00010            )%%00010            return None%%00010%%00010%%00010def data_to_df(report_data, date):%%00010    &quot;&quot;&quot; Parse and pre-process Data%%00010    &quot;&quot;&quot;%%00010    data = parse(report_data)%%00010    data['load_timestamp'] = datetime.datetime.now() %%00010    #data[&quot;data_date&quot;] = date%%00010    return data%%00010%%00010# date_range = pd.date_range('2019-01-01', pd.datetime.today()).strftime('%Y-%m-%d').to_list()%%00010report_list = []%%00010clean_data = pd.DataFrame()%%00010yesterday = pd.datetime.today() - pd.Timedelta(&quot;1 day&quot;)%%00010%%00010# For daily data get last 7 days, no regard for day of week%%00010# For weekly data get Monday to Sunday data of last complete week%%00010start_dates = pd.date_range(DATE_FIRST_EMPTY_DAY, yesterday, freq=PERIOD_FREQ, closed=None)%%00010start_dates = [%%00010    date for date in start_dates if (date + pd.Timedelta(&quot;6 day&quot;)) &lt;= yesterday%%00010]%%00010end_dates = [%%00010    (date + pd.Timedelta(&quot;6 day&quot;)).strftime(&quot;%Y-%m-%d&quot;) for date in start_dates%%00010]%%00010start_dates = [date.strftime(&quot;%Y-%m-%d&quot;) for date in start_dates]%%00010%%00010# For testing only do a few entries%%00010if TEST==1:%%00010%%00009start_dates = start_dates[0:2]%%00010%%00009end_dates = end_dates[0:2]%%00010%%00010%%00009%%00010for date_from, date_to in zip(start_dates, end_dates):%%00010    &quot;&quot;&quot; Send all Reports to queue%%00010    &quot;&quot;&quot;%%00010    report_id = queue_report(date_from, date_to)%%00010    report_list.append((report_id, date_from))%%00010%%00010tmp = []%%00010for report in report_list:%%00010    &quot;&quot;&quot; Retrieve all reports and process them to df%%00010        deal with empty requests%%00010    &quot;&quot;&quot;%%00010    report_data = get_report(report[0])%%00010    tmp.append(report_data)%%00010    if float(report_data.get(&quot;report&quot;).get(&quot;totals&quot;)[0]) &gt; 0:%%00010        data_df = data_to_df(report_data, report[1])%%00010        clean_data = clean_data.append(data_df)%%00010    else:%%00010        log.error(f&quot;Request {report[0]} for {report[1]} returned empty&quot;)%%00010%%00010%%00010# To Knime%%00010clean_data = clean_data.drop_duplicates(keep=&quot;first&quot;)%%00010# Bug in Knime when exporting: Related to DateTime%%00010clean_data[&quot;load_timestamp&quot;] = clean_data[&quot;load_timestamp&quot;].astype(str)%%00010clean_data = clean_data.apply(lambda x: pd.to_numeric(x, errors=&quot;ignore&quot;))%%00010%%00010# Clean Chars%%00010clean_data = clean_data.applymap(lambda x:%%00010                         x.encode('iso8859-15', 'replace')\%%00010                         .decode('iso8859-15') if isinstance(x, str) else x)%%00010output_table = clean_data.reset_index(drop=True)%%00010%%00010"/>
<entry key="rowLimit" type="xint" value="1000"/>
<entry key="pythonVersionOption" type="xstring" value="python3"/>
<entry key="python2Command" type="xstring" value=""/>
<entry key="python3Command" type="xstring" value=""/>
<entry key="chunkSize" type="xint" value="500000"/>
<entry key="convertMissingToPython" type="xboolean" value="false"/>
<entry key="convertMissingFromPython" type="xboolean" value="false"/>
<entry key="sentinelOption" type="xstring" value="MIN_VAL"/>
<entry key="sentinelValue" type="xint" value="0"/>
</config>
<config key="nodeAnnotation">
<entry key="text" type="xstring" value="Request Adobe%%00013%%00010Report from API"/>
<entry key="bgcolor" type="xint" value="16777215"/>
<entry key="x-coordinate" type="xint" value="227"/>
<entry key="y-coordinate" type="xint" value="2899"/>
<entry key="width" type="xint" value="106"/>
<entry key="height" type="xint" value="30"/>
<entry key="alignment" type="xstring" value="CENTER"/>
<entry key="borderSize" type="xint" value="0"/>
<entry key="borderColor" type="xint" value="16777215"/>
<entry key="defFontSize" type="xint" value="9"/>
<entry key="annotation-version" type="xint" value="20151123"/>
<config key="styles"/>
</config>
<entry key="customDescription" type="xstring" isnull="true" value=""/>
<entry key="state" type="xstring" value="CONFIGURED"/>
<entry key="factory" type="xstring" value="org.knime.python2.nodes.source.Python2SourceNodeFactory"/>
<entry key="node-name" type="xstring" value="Python Source"/>
<entry key="node-bundle-name" type="xstring" value="KNIME Python nodes"/>
<entry key="node-bundle-symbolic-name" type="xstring" value="org.knime.python2.nodes"/>
<entry key="node-bundle-vendor" type="xstring" value="KNIME AG, Zurich, Switzerland"/>
<entry key="node-bundle-version" type="xstring" value="4.1.3.v202005112253"/>
<entry key="node-feature-name" type="xstring" value="KNIME Python Integration"/>
<entry key="node-feature-symbolic-name" type="xstring" value="org.knime.features.python2.feature.group"/>
<entry key="node-feature-vendor" type="xstring" value="KNIME AG, Zurich, Switzerland"/>
<entry key="node-feature-version" type="xstring" value="4.1.3.v202005112253"/>
<config key="factory_settings"/>
<entry key="name" type="xstring" value="Python Source"/>
<entry key="hasContent" type="xboolean" value="false"/>
<entry key="isInactive" type="xboolean" value="false"/>
<config key="ports">
<config key="port_1">
<entry key="index" type="xint" value="1"/>
<entry key="port_dir_location" type="xstring" isnull="true" value=""/>
</config>
</config>
<config key="filestores">
<entry key="file_store_location" type="xstring" isnull="true" value=""/>
<entry key="file_store_id" type="xstring" isnull="true" value=""/>
</config>
</config>
